{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c17d8063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59ad22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DC_CLOUD = 0\n",
    "DC_EDGE = 1\n",
    "\n",
    "DS_PUBLIC = 0\n",
    "DS_PRIVATE = 1\n",
    "\n",
    "DS_INIT = 0\n",
    "DS_GEN = 1\n",
    "\n",
    "NUM_DATACENTRES = 3\n",
    "NUM_DATASETS = 5\n",
    "NUM_TOTAL_DATASETS = 6\n",
    "NUM_TASKS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba18508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class datacentre(object):\n",
    "    def __init__(self, dc_type: int, capacity: int, index: int):\n",
    "        self.capacity = capacity\n",
    "        self.dc_type = dc_type\n",
    "        self.index = index\n",
    "        \n",
    "    def __str__(self):\n",
    "        return (str(self.capacity))\n",
    "\n",
    "class dataset(object):\n",
    "    def __init__(self, size: int, gt: int, lc: datacentre, flc: datacentre,\n",
    "                 ds_type: int, input_ds_type: int, index: int):\n",
    "        self.size = size\n",
    "        self.gt = gt #Generating task\n",
    "        self.lc = lc #dc location\n",
    "        self.flc = flc #dc final location\n",
    "        self.ds_type = ds_type #public or private\n",
    "        self.input_ds_type = input_ds_type #input or generated dataset\n",
    "        self.index = index\n",
    "\n",
    "    def __str__(self):\n",
    "        return (str(self.size) + str(self.ds_type))\n",
    "        \n",
    "        \n",
    "class task(object):\n",
    "    def __init__(self, ids: List[int] , ods: List[int], index: int):\n",
    "        self.ids = ids #input dataasets\n",
    "        self.ods = ods #output dataset\n",
    "        self.index = index\n",
    "        \n",
    "    def __str__(self):\n",
    "        return (str(self.ids) + \" \" + str(self.ods))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "724c3b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacentres = []\n",
    "datasets = []\n",
    "tasks = []\n",
    "task_dependencies = []\n",
    "bandwidth = []\n",
    "\n",
    "def initialize_dc():\n",
    "    dc1 = datacentre(DC_CLOUD, 10000, 1)\n",
    "    dc2 = datacentre(DC_EDGE, 20, 2)\n",
    "    dc3 = datacentre(DC_EDGE, 20, 3)\n",
    "    \n",
    "    datacentres.extend([dc1, dc2, dc3])\n",
    "    \n",
    "def initialize_ds():\n",
    "    ds1 = dataset(3, None, -1, -1, DS_PUBLIC, DS_INIT, 1)\n",
    "    ds2 = dataset(5, None, -1, -1, DS_PUBLIC, DS_INIT, 2)\n",
    "    ds3 = dataset(3, None, -1, -1, DS_PUBLIC, DS_INIT, 3)\n",
    "    ds4 = dataset(3, None, datacentres[1], datacentres[1], DS_PRIVATE, DS_INIT, 4)\n",
    "    ds5 = dataset(5, None, datacentres[2], datacentres[2], DS_PRIVATE, DS_INIT, 5)\n",
    "    ds6 = dataset(8, None, -1, -1, DS_PUBLIC, DS_GEN,6)\n",
    "    \n",
    "    datasets.extend([ds1, ds2, ds3, ds4, ds5, ds6])\n",
    "    \n",
    "\n",
    "def initialize_tasks():\n",
    "    t1 = task([datasets[0], datasets[1]], [], 1)\n",
    "    t2 = task([datasets[0], datasets[1], datasets[5]], [], 2)\n",
    "    t3 = task([datasets[0], datasets[1], datasets[2], datasets[5]], [], 3)\n",
    "    t4 = task([datasets[2], datasets[3], datasets[5]], [], 4)\n",
    "    t5 = task([datasets[4]], [datasets[5]], 5)\n",
    "    \n",
    "    tasks.extend([t1, t2, t3, t4, t5])\n",
    "    \n",
    "\n",
    "def initialize_task_dep():\n",
    "    global task_dependencies\n",
    "    task_dependencies = [[] for i in range(NUM_TASKS)]\n",
    "    task_dependencies[4].extend([1,2,3])\n",
    "    \n",
    "    \n",
    "def initialize_bandwidth():\n",
    "    global bandwidth\n",
    "    bandwidth = [[0] * NUM_DATACENTRES for i in range(NUM_DATACENTRES)]\n",
    "    bandwidth[0][1] = bandwidth[1][0] = 10\n",
    "    bandwidth[0][2] = bandwidth[2][0] = 20\n",
    "    bandwidth[1][2] = bandwidth[2][1] = 150\n",
    "    \n",
    "initialize_dc()\n",
    "initialize_ds()\n",
    "initialize_tasks()\n",
    "initialize_task_dep()\n",
    "initialize_bandwidth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce322fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d5129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4f6b4ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "[4, 3, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "#Topologically sort tasks\n",
    "#Place task in datacentre with minimal transmission time\n",
    "#After placing task place output tasks in same data centre\n",
    "#Calculate Ttotal\n",
    "\n",
    "    \n",
    "# A recursive function used by topologicalSort\n",
    "def topologicalSortUtil(v,visited,stack):\n",
    "\n",
    "    visited[v] = True\n",
    "\n",
    "    for i in task_dependencies[v]:\n",
    "        print(i)\n",
    "        if visited[i] == False:\n",
    "            topologicalSortUtil(i,visited,stack)\n",
    "\n",
    "    stack.insert(0,v)\n",
    "\n",
    "def topologicalSort():\n",
    "\n",
    "    visited = [False]*NUM_TASKS\n",
    "    stack =[]\n",
    "\n",
    "    for i in range(NUM_TASKS):\n",
    "        if visited[i] == False:\n",
    "            topologicalSortUtil(i,visited,stack)\n",
    "\n",
    "    print(stack)\n",
    "    return stack\n",
    "        \n",
    "sorted_tasks = topologicalSort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54008faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6c32a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_placement_feasibility(task, task_datacentre, datacentre_usage):\n",
    "    \n",
    "    for ids in task.ids:\n",
    "        print(\"Dataset is \" + str(ids.index) + \" and size is \" + str(ids.size))\n",
    "        if(datacentre_usage[task_datacentre.index-1] + ids.size > task_datacentre.capacity):\n",
    "            return False, task_datacentre\n",
    "        datacentre_usage[task_datacentre.index-1] += ids.size\n",
    "        print(\"Usage of data centre \" + str(task_datacentre.index) + \" increased to \" \n",
    "              + str( datacentre_usage[task_datacentre.index-1]))\n",
    "        \n",
    "    for ods in task.ods:\n",
    "        print(\"Dataset is \" + str(ods.index) + \" and size is \" + str(ods.size))\n",
    "        if(datacentre_usage[task_datacentre.index-1] + ods.size > task_datacentre.capacity):\n",
    "            return False, task_datacentre\n",
    "        datacentre_usage[task_datacentre.index-1] += ods.size\n",
    "\n",
    "        print(\"Usage of data centre \" + str(task_datacentre.index) + \" increased to \" \n",
    "              + str( datacentre_usage[task_datacentre.index-1]))\n",
    "        \n",
    "        \n",
    "    return True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a1b996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c89d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d263ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "bef3e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_tasks(particle, datacentre_usage):\n",
    "    #Map from dataset to datacentre index (1 indexed)\n",
    "    dataset_to_datacenter = [0] * NUM_TOTAL_DATASETS\n",
    "    \n",
    "    #Copy initial particle dataset to datacnetre map to this map\n",
    "    for i in range(len(particle)):\n",
    "        dataset_to_datacenter[i] = particle[i]\n",
    "    print(dataset_to_datacenter)\n",
    "        \n",
    "        \n",
    "    task_datacentres = [0] * NUM_TASKS\n",
    "    transmission_times = [0] * NUM_TASKS\n",
    "    \n",
    "    #Iterate through tasks in topologically sorted order\n",
    "    for i in range(NUM_TASKS):\n",
    "        \n",
    "        transmission_time = 10000\n",
    "        datacentre = None\n",
    "        \n",
    "        task = tasks[sorted_tasks[i]]\n",
    "        \n",
    "        #Check if the task requires any private dataset\n",
    "        private_dataset_input = 0\n",
    "        for ids in task.ids:\n",
    "            if(ids.ds_type == DS_PRIVATE):\n",
    "                datacentre = ids.lc\n",
    "                private_dataset_input = 1\n",
    "                print(\"Private ids \" + str(ids.index) + \" found in datacenter \" + \n",
    "                      str(dataset_to_datacenter[ids.index-1]))\n",
    "        \n",
    "        #Private dataset required for this task, hence task should be placed in\n",
    "        # datacentre in which private dataset is present\n",
    "        \n",
    "        if(private_dataset_input == 1):\n",
    "            transmission_time_i = 0\n",
    "            \n",
    "            #Iterate through the input datasets of task to determine the other datasets\n",
    "            # which should be transmitted to this datacentre\n",
    "            \n",
    "            for ids in task.ids:\n",
    "                print(\"Input dataset \" + str(ids.index))\n",
    "                idc = dataset_to_datacenter[ids.index-1]\n",
    "                print(\"Input dataset resides in \" + str(idc))\n",
    "                if(idc != (datacentre.index)):\n",
    "                    #print(str(idc) + \" \" + str(datacentre.index))\n",
    "                    transmission_time_i += ids.size * 1024 / (bandwidth[idc-1][datacentre.index-1])\n",
    "                    print(\"Transmitting dataset \" + str(ids.index) +  \" to datacentre \" + str(datacentre.index))\n",
    "                    \n",
    "            #Iterate through output datasets of task to place output dataset in the particular datacentre\n",
    "            for ods in task.ods:\n",
    "                dataset_to_datacenter[ods.index-1] = datacentre.index\n",
    "                print(\"Adding output dataset \" + str(ods.index) + \" to datacentre \" + str(datacentre.index))\n",
    "                \n",
    "            transmission_times[task.index-1] = (transmission_time_i)\n",
    "            task_datacentres[task.index-1] = (datacentre.index)\n",
    "            \n",
    "            #Check if after bringing in input datasets and generating output datasets will violate the dataentre\n",
    "            #storage constraints\n",
    "            feasibility, invalid_datacentre = task_placement_feasibility(task, datacentre, datacentre_usage)\n",
    "            if(feasibility == False):\n",
    "                return False, -1, [], invalid_datacentre\n",
    "            \n",
    "            \n",
    "        #No private dataset in input dataset of task\n",
    "        #Need to iterate through all daatacentres and choose the one with minimal transmission time\n",
    "        \n",
    "        else:\n",
    "            transmission_time_i = 10000\n",
    "            datacentre = None\n",
    "            \n",
    "            #Iterating through datacentres\n",
    "            for j in range(NUM_DATACENTRES):\n",
    "                task_dc = datacentres[j]\n",
    "                transmission_time_dcj = 0\n",
    "                print(\"Placing task in datacentre \" + str(task_dc.index))\n",
    "          \n",
    "                #Iterate through the input datasets of task to determine the other datasets\n",
    "                # which should be transmitted to this datacentre\n",
    "            \n",
    "                for ids in task.ids:\n",
    "                    print(\"Input dataset \" + str(ids.index))\n",
    "                    idc = dataset_to_datacenter[ids.index-1]\n",
    "                    print(\"Input dataset resides in \" + str(idc))\n",
    "                    if(idc != task_dc.index):\n",
    "                        transmission_time_dcj += ids.size * 1024 / (bandwidth[idc-1][task_dc.index-1])\n",
    "                        print(\"Transmitting dataset \" + str(ids.index) +  \" to datacentre \" + str(task_dc.index) \n",
    "                             + \" in \" + str(ids.size * 1024 / (bandwidth[idc-1][task_dc.index-1])))\n",
    "                   \n",
    "                print(\"Trans time for task \" + str(task.index) + \" in datacentre \" + str(j+1) + \" is \" \n",
    "                      + str(transmission_time_dcj))\n",
    "                \n",
    "                #Record the datacentre which produces minimal transmission time\n",
    "                if (transmission_time_dcj < transmission_time_i):\n",
    "                    print(\"Here \" + str(task_dc.index))\n",
    "                    transmission_time_i = transmission_time_dcj\n",
    "                    datacentre = task_dc\n",
    "                    \n",
    "            #Iterate through output datasets of task to place output dataset in the particular datacentre\n",
    "            for ods in task.ods:\n",
    "                dataset_to_datacenter[ods.index-1] = datacentre.index-1\n",
    "                print(\"Adding output dataset \" + str(ods.index) + \" to datacentre \" + str(datacentre.index))\n",
    "                \n",
    "            print(\"Appending \" + str(transmission_time_i) + \" \" + str(datacentre.index))\n",
    "            transmission_times[task.index-1] = (transmission_time_i)\n",
    "            task_datacentres[task.index-1] = (datacentre.index)\n",
    "            \n",
    "            #Check if after bringing in input datasets and generating output datasets will violate the dataentre\n",
    "            #storage constraints\n",
    "            feasibility, invalid_datacentre = task_placement_feasibility(task, datacentre, datacentre_usage)\n",
    "            if(feasibility == False):\n",
    "                return False, -1, [], invalid_datacentre\n",
    "            \n",
    "    print(transmission_times)\n",
    "    print(task_datacentres)  \n",
    "    \n",
    "    return True, transmission_times, task_datacentres, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6433212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bcc6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064b18a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb36855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a038750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_particle_feasibility(particle):\n",
    "    \n",
    "    datacentre_usage = [0] * NUM_DATACENTRES\n",
    "        \n",
    "    for i in range(len(particle)):\n",
    "        if((datacentre_usage[particle[i]-1] + datasets[i].size) > datacentres[particle[i]-1].capacity):\n",
    "            return False, datacentres[particle[i]-1]\n",
    "        datacentre_usage[particle[i]-1] += datasets[i].size \n",
    "        print(\"Data centre usage of data centre \" + str(datacentres[particle[i]-1].index) + \" is \" + str(datacentre_usage[particle[i] - 1]))\n",
    "        \n",
    "    return True, None\n",
    "\n",
    "def fitness(particle):\n",
    "    \n",
    "    initial_feasibility, invalid_datacentre = initial_particle_feasibility(particle)\n",
    "    if(initial_feasibility == False):\n",
    "        return 1, 10000, invalid_datacentre\n",
    "\n",
    "    feasibility, total_transmission_time, task_datacentres, invalid_datacentre = place_tasks(particle, [0] * NUM_DATACENTRES)\n",
    "    \n",
    "    if(feasibility == False):\n",
    "        return 1, 10000, invalid_datacentre\n",
    "\n",
    "    return 0, np.sum(total_transmission_time), None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af33ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "154c4c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data centre usage of data centre 1 is 3\n",
      "Data centre usage of data centre 1 is 8\n",
      "Data centre usage of data centre 1 is 11\n",
      "Data centre usage of data centre 2 is 3\n",
      "Data centre usage of data centre 3 is 5\n",
      "[1, 1, 1, 2, 3, 0]\n",
      "Private ids 5 found in datacenter 3\n",
      "Input dataset 5\n",
      "Input dataset resides in 3\n",
      "Adding output dataset 6 to datacentre 3\n",
      "Dataset is 5 and size is 5\n",
      "Usage of data centre 3 increased to 5\n",
      "Dataset is 6 and size is 8\n",
      "Data centre 3 violating storage constraint\n"
     ]
    }
   ],
   "source": [
    "feasibility, fitness_score, invalid_datacentre = fitness([1,1,1,2,3])\n",
    "\n",
    "if(feasibility == 0):\n",
    "    print(\"Fitness score is \" + str(fitness_score))\n",
    "else:\n",
    "    print(\"Data centre \" + str(invalid_datacentre.index) + \" violating storage constraint\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8f3505d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pep8 standards - https://www.python.org/dev/peps/pep-0008/#id39 \n",
    "# Enum -https://docs.python.org/3/library/enum.html#creating-an-enum \n",
    "# Keyword arguments for Dataset (any class with large number of args)\n",
    "# Pass arrays to function instead of global\n",
    "# Timesheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3c407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe0b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e267e4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e4be92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6123628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06111a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08805837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c8d5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58510fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc87c9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b575f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c961cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_public_dataset_positions(datasets):\n",
    "    public_datasets_positions = []\n",
    "    for i in range(len(datasets)):\n",
    "        if(datasets[i].ds_type == DS_PUBLIC and datasets[i].input_ds_type == DS_INIT):\n",
    "            public_datasets_positions.append(i)\n",
    "    \n",
    "    return public_datasets_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "cd602554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "public_datasets_positions = get_public_dataset_positions(datasets)\n",
    "print(public_datasets_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3328ddd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8e9b858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "558c1932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_integer(low, high, K = -1):\n",
    "    r = random.randint(low, high)\n",
    "    while(r == K):\n",
    "        r = random.randint(low, high)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "313fd9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutation(particle, w, public_dataset_positions):\n",
    "    \n",
    "    r = random.uniform(0, 1)\n",
    "    print(r)\n",
    "    if (r > w):\n",
    "        return particle\n",
    "    \n",
    "    feasibility, fitness_score, invalid_datacentre = fitness(particle)\n",
    "    print(\"In mutation\")\n",
    "    if(feasibility == 0):\n",
    "        print(\"Fitness score is \" + str(fitness_score))\n",
    "        mutation_index = random.choice(public_dataset_positions)\n",
    "        print(\"Mutation index is \" + str(mutation_index))\n",
    "        random_datacentre = random_integer(1, NUM_DATACENTRES, particle[mutation_index])\n",
    "        particle[mutation_index] = random_datacentre\n",
    "        \n",
    "    else:\n",
    "        print(\"Data centre \" + str(invalid_datacentre.index) + \" violating storage constraint\") \n",
    "        for i in range(len(particle)):\n",
    "            if(particle[i] == invalid_datacentre and datasets[i].ds_type == DS_PUBLIC):\n",
    "                random_datacentre = random_integer(1, NUM_DATACENTRES, particle[i])\n",
    "                particle[mutation_index] = random_datacentre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "ce457c8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3796421845601786\n",
      "Data centre usage of data centre 1 is 3\n",
      "Data centre usage of data centre 1 is 8\n",
      "Data centre usage of data centre 1 is 11\n",
      "Data centre usage of data centre 2 is 3\n",
      "Data centre usage of data centre 3 is 5\n",
      "[1, 1, 1, 2, 3, 0]\n",
      "Private ids 5 found in datacenter 3\n",
      "Input dataset 5\n",
      "Input dataset resides in 3\n",
      "Adding output dataset 6 to datacentre 3\n",
      "Dataset is 5 and size is 5\n",
      "Usage of data centre 3 increased to 5\n",
      "Dataset is 6 and size is 8\n",
      "Usage of data centre 3 increased to 13\n",
      "Private ids 4 found in datacenter 2\n",
      "Input dataset 3\n",
      "Input dataset resides in 1\n",
      "Transmitting dataset 3 to datacentre 2\n",
      "Input dataset 4\n",
      "Input dataset resides in 2\n",
      "Input dataset 6\n",
      "Input dataset resides in 3\n",
      "Transmitting dataset 6 to datacentre 2\n",
      "Dataset is 3 and size is 3\n",
      "Usage of data centre 2 increased to 3\n",
      "Dataset is 4 and size is 3\n",
      "Usage of data centre 2 increased to 6\n",
      "Dataset is 6 and size is 8\n",
      "Usage of data centre 2 increased to 14\n",
      "Placing task in datacentre 1\n",
      "Input dataset 1\n",
      "Input dataset resides in 1\n",
      "Input dataset 2\n",
      "Input dataset resides in 1\n",
      "Input dataset 3\n",
      "Input dataset resides in 1\n",
      "Input dataset 6\n",
      "Input dataset resides in 3\n",
      "Transmitting dataset 6 to datacentre 1 in 409.6\n",
      "Trans time for task 3 in datacentre 1 is 409.6\n",
      "Here 1\n",
      "Placing task in datacentre 2\n",
      "Input dataset 1\n",
      "Input dataset resides in 1\n",
      "Transmitting dataset 1 to datacentre 2 in 307.2\n",
      "Input dataset 2\n",
      "Input dataset resides in 1\n",
      "Transmitting dataset 2 to datacentre 2 in 512.0\n",
      "Input dataset 3\n",
      "Input dataset resides in 1\n",
      "Transmitting dataset 3 to datacentre 2 in 307.2\n",
      "Input dataset 6\n",
      "Input dataset resides in 3\n",
      "Transmitting dataset 6 to datacentre 2 in 54.61333333333334\n",
      "Trans time for task 3 in datacentre 2 is 1181.0133333333333\n",
      "Placing task in datacentre 3\n",
      "Input dataset 1\n",
      "Input dataset resides in 1\n",
      "Transmitting dataset 1 to datacentre 3 in 153.6\n",
      "Input dataset 2\n",
      "Input dataset resides in 1\n",
      "Transmitting dataset 2 to datacentre 3 in 256.0\n",
      "Input dataset 3\n",
      "Input dataset resides in 1\n",
      "Transmitting dataset 3 to datacentre 3 in 153.6\n",
      "Input dataset 6\n",
      "Input dataset resides in 3\n",
      "Trans time for task 3 in datacentre 3 is 563.2\n",
      "Appending 409.6 1\n",
      "Dataset is 1 and size is 3\n",
      "Usage of data centre 1 increased to 3\n",
      "Dataset is 2 and size is 5\n",
      "Usage of data centre 1 increased to 8\n",
      "Dataset is 3 and size is 3\n",
      "Usage of data centre 1 increased to 11\n",
      "Dataset is 6 and size is 8\n",
      "Usage of data centre 1 increased to 19\n",
      "Placing task in datacentre 1\n",
      "Input dataset 1\n",
      "Input dataset resides in 1\n",
      "Input dataset 2\n",
      "Input dataset resides in 1\n",
      "Input dataset 6\n",
      "Input dataset resides in 3\n",
      "Transmitting dataset 6 to datacentre 1 in 409.6\n",
      "Trans time for task 2 in datacentre 1 is 409.6\n",
      "Here 1\n",
      "Placing task in datacentre 2\n",
      "Input dataset 1\n",
      "Input dataset resides in 1\n",
      "Transmitting dataset 1 to datacentre 2 in 307.2\n",
      "Input dataset 2\n",
      "Input dataset resides in 1\n",
      "Transmitting dataset 2 to datacentre 2 in 512.0\n",
      "Input dataset 6\n",
      "Input dataset resides in 3\n",
      "Transmitting dataset 6 to datacentre 2 in 54.61333333333334\n",
      "Trans time for task 2 in datacentre 2 is 873.8133333333334\n",
      "Placing task in datacentre 3\n",
      "Input dataset 1\n",
      "Input dataset resides in 1\n",
      "Transmitting dataset 1 to datacentre 3 in 153.6\n",
      "Input dataset 2\n",
      "Input dataset resides in 1\n",
      "Transmitting dataset 2 to datacentre 3 in 256.0\n",
      "Input dataset 6\n",
      "Input dataset resides in 3\n",
      "Trans time for task 2 in datacentre 3 is 409.6\n",
      "Appending 409.6 1\n",
      "Dataset is 1 and size is 3\n",
      "Usage of data centre 1 increased to 22\n",
      "Dataset is 2 and size is 5\n",
      "Usage of data centre 1 increased to 27\n",
      "Dataset is 6 and size is 8\n",
      "Usage of data centre 1 increased to 35\n",
      "Placing task in datacentre 1\n",
      "Input dataset 1\n",
      "Input dataset resides in 1\n",
      "Input dataset 2\n",
      "Input dataset resides in 1\n",
      "Trans time for task 1 in datacentre 1 is 0\n",
      "Here 1\n",
      "Placing task in datacentre 2\n",
      "Input dataset 1\n",
      "Input dataset resides in 1\n",
      "Transmitting dataset 1 to datacentre 2 in 307.2\n",
      "Input dataset 2\n",
      "Input dataset resides in 1\n",
      "Transmitting dataset 2 to datacentre 2 in 512.0\n",
      "Trans time for task 1 in datacentre 2 is 819.2\n",
      "Placing task in datacentre 3\n",
      "Input dataset 1\n",
      "Input dataset resides in 1\n",
      "Transmitting dataset 1 to datacentre 3 in 153.6\n",
      "Input dataset 2\n",
      "Input dataset resides in 1\n",
      "Transmitting dataset 2 to datacentre 3 in 256.0\n",
      "Trans time for task 1 in datacentre 3 is 409.6\n",
      "Appending 0 1\n",
      "Dataset is 1 and size is 3\n",
      "Usage of data centre 1 increased to 38\n",
      "Dataset is 2 and size is 5\n",
      "Usage of data centre 1 increased to 43\n",
      "[0, 409.6, 409.6, 361.81333333333333, 0]\n",
      "[1, 1, 1, 2, 3]\n",
      "In mutation\n",
      "Fitness score is 1181.0133333333333\n",
      "Mutation index is 2\n",
      "[1, 1, 3, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "particle = [1,1,1,2,3]\n",
    "mutation(particle, 0.9, [0,1,2])\n",
    "print(particle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "8a51c96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_private_dataset_locations(particle):\n",
    "    for i in range(len(particle)):\n",
    "        if(datasets[i].ds_type == DS_PRIVATE):\n",
    "            particle[i] = datasets[i].lc.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "e9f0aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover(particle1, particle2, c):\n",
    "    r = random.uniform(0, 1)\n",
    "    print(r)\n",
    "    if (r > c):\n",
    "        return particle\n",
    "    \n",
    "    new_particle = [0] * NUM_DATASETS\n",
    "    index1 = random_integer(0, len(particle1) - 1)\n",
    "    index2 = random_integer(0, len(particle1) - 1, index1)\n",
    "    \n",
    "    print(\"Indexes are \" + str(index1) + \" \" + str(index2))\n",
    "    \n",
    "    if(index1 > index2):\n",
    "        temp = index1\n",
    "        index1 = index2\n",
    "        index2 = temp\n",
    "    \n",
    "    for i in range(len(particle1)):\n",
    "        if(i < index1):\n",
    "            new_particle[i] = particle1[i]\n",
    "        elif (i <= index2):\n",
    "            new_particle[i] = particle2[i]\n",
    "        else:\n",
    "            new_particle[i] = particle1[i]\n",
    "    \n",
    "    #fix_private_dataset_locations(new_particle)\n",
    "    \n",
    "    return new_particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "1556bd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09699816827703278\n",
      "Indexes are 0 4\n",
      "[2, 3, 3, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "new_particle = crossover([1,1,1,2,3], [2,3,3,2,3], 0.8)\n",
    "print(new_particle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1182f9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e26a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb141f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "30ab3207",
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_POPULATION = 10\n",
    "\n",
    "MAX_W = 0.9\n",
    "MIN_W = 0.4\n",
    "\n",
    "C1_START = 0.9\n",
    "C1_END = 0.4\n",
    "\n",
    "C2_START = 0.9\n",
    "C2_END = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "8a270c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 2 2 3]\n",
      "[3 1 1 2 3]\n",
      "[2 2 2 2 3]\n",
      "[2 3 1 2 3]\n",
      "[1 2 3 2 3]\n",
      "[3 2 1 2 3]\n",
      "[1 3 3 2 3]\n",
      "[3 1 3 2 3]\n",
      "[3 1 2 2 3]\n",
      "[1 1 3 2 3]\n"
     ]
    }
   ],
   "source": [
    "particles = []\n",
    "def initialize_population():\n",
    "    for i in range(INITIAL_POPULATION):\n",
    "        particle = np.random.randint(1, NUM_DATACENTRES+1, NUM_DATASETS)\n",
    "        fix_private_dataset_locations(particle)\n",
    "        particles.append(particle)\n",
    "        print(particle)\n",
    "        \n",
    "initialize_population()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04218b08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
